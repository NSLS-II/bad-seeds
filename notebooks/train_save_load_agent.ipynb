{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "import tensorforce\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.environments import Environment\n",
    "from tensorforce.execution import Runner\n",
    "\n",
    "from bad_seeds.simple.bad_seeds_04_bollux import Bollux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorforce.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent_, bad_seeds_env_):\n",
    "    # Evaluate for 100 episodes\n",
    "    sum_rewards = 0.0\n",
    "    for _ in range(100):\n",
    "        states = bad_seeds_env_.reset()\n",
    "        internals = agent_.initial_internals()\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            actions, internals = agent_.act(\n",
    "                states=states,\n",
    "                internals=internals,\n",
    "                independent=True,\n",
    "                deterministic=True\n",
    "            )\n",
    "            states, terminal, reward = bad_seeds_env_.execute(actions=actions)\n",
    "            sum_rewards += reward\n",
    "\n",
    "    average_evaluation_reward = sum_rewards / 100\n",
    "    print(f'Mean episode reward: {average_evaluation_reward}')\n",
    "\n",
    "    return average_evaluation_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_seeds_environment = Environment.create(\n",
    "    environment=Bollux,\n",
    "    seed_count=10,\n",
    "    reward_probability=2/3,\n",
    "    bad_seed_count=3,\n",
    "    max_episode_length=100,\n",
    "    max_episode_timesteps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "variable_noise = 0.1\n",
    "l2_regularization = 0.01\n",
    "\n",
    "agent = Agent.create(\n",
    "    agent=\"ppo\",\n",
    "    environment=bad_seeds_environment,\n",
    "\n",
    "    batch_size=batch_size,\n",
    "    variable_noise=0.1,\n",
    "\n",
    "    l2_regularization=l2_regularization,\n",
    "\n",
    "    summarizer=dict(\n",
    "        directory=f\"training_data_saveload/agent_ppo_01_env_bollux_bs{batch_size}_l2r{l2_regularization}/summaries\",\n",
    "        labels=[\"graph\", \"entropy\", \"kl-divergence\", \"losses\", \"rewards\"],  # tensorforce 0.5.5\n",
    "        #summaries=[\"entropy\", \"kl-divergence\", \"loss\", \"reward\"],  # removed \"graph\"  # tensorforce 0.6.0\n",
    "        flush=True,\n",
    "        # frequency=100  not necessary?\n",
    "    ),\n",
    ")\n",
    "\n",
    "agent.debug = False\n",
    "\n",
    "print(\"begin training\")\n",
    "runner = Runner(agent=agent, environment=bad_seeds_environment)\n",
    "runner.run(num_episodes=1000)\n",
    "print(\"done training\")\n",
    "\n",
    "agent.debug = True\n",
    "\n",
    "print(\"begin evaluation\")\n",
    "runner = Runner(agent=agent, environment=bad_seeds_environment)\n",
    "runner.run(num_episodes=1000, evaluation=True)\n",
    "#avg_reward = evaluate_agent(agent_=agent, bad_seeds_env_=bad_seeds_environment)\n",
    "#print(f\"average reward: {avg_reward}\")\n",
    "print(\"done evaluating\")\n",
    "\n",
    "agent.save(directory=\"saved_models\", format=\"numpy\")\n",
    "\n",
    "agent.close()\n",
    "bad_seeds_environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bad_seeds_environment = Environment.create(\n",
    "    environment=Bollux,\n",
    "    seed_count=10,\n",
    "    bad_seed_count=3,\n",
    "    reward_probability=1.0,\n",
    "    max_episode_length=100,\n",
    "    max_episode_timesteps=100,\n",
    ")\n",
    "\n",
    "loaded_agent = Agent.load(\n",
    "    directory=\"saved_models\",\n",
    "    format=\"numpy\",\n",
    "    environment=new_bad_seeds_environment,\n",
    "    #summarizer=dict(\n",
    "    #    directory=f\"training_data_saveload/agent_ppo_01_env_bollux_bs{batch_size}_l2r{l2_regularization}/summaries\",\n",
    "    #    labels=[\"graph\", \"entropy\", \"kl-divergence\", \"losses\", \"rewards\"],  # tensorforce 0.5.5\n",
    "    #    #summaries=[\"entropy\", \"kl-divergence\", \"loss\", \"reward\"],  # removed \"graph\"  # tensorforce 0.6.0\n",
    "    #    flush=True,\n",
    "    #    # frequency=100  not necessary?\n",
    "    #),\n",
    ")\n",
    "\n",
    "loaded_agent.debug = True\n",
    "\n",
    "loaded_agent_runner = Runner(\n",
    "    agent=loaded_agent,\n",
    "    environment=new_bad_seeds_environment\n",
    ")\n",
    "\n",
    "loaded_agent_runner.run(\n",
    "    num_episodes=1000,\n",
    "    evaluation=True\n",
    ")\n",
    "#loaded_agent_runner.close()\n",
    "\n",
    "#avg_reward = evaluate_agent(loaded_agent, new_bad_seeds_environment)\n",
    "#print(f\"average reward: {avg_reward}\")\n",
    "\n",
    "loaded_agent.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}